元宇宙的整体框架：元宇宙=立体互联网+价值互联网。

要用计算机技术创造一个三维立体的、进而可以走入其中的数字世界。这个数字世界可能是完全虚拟的，也可能是叠加在实体世界之上的，半数字、半实体的世界。

Facebook、微软、苹果都在大力探索元宇宙的立体互联网部分，但它们的侧重点略有不同: Facebook强调VR（虚拟现实），微软发展MR（混合现实），苹果则更加重视AR（增强现实）。

> [!IMPORTANT]
>
> 想要用计算机创造一个三维立体的数字世界，工程师要完成的任务可以拆解为三大类：“扫描实体世界”、“渲染数字世界”和“操控数字世界”。
>
> “扫描实体世界”是将真实的世界数字化、建立物体的三维模型；
>
> “渲染数字世界”是基于模型和材质，用计算机渲染出可以欺骗双眼的图像；
>
> “操控数字世界”则是让我们能够与这个计算机创造出来的三维世界互动。

**用计算机构建一个虚拟世界时，其实并不是完全虚构的，多数时候，需要构建的是一个实体世界的翻版。**

即便是哈利波特这样的奇幻故事，它里面的“九又四分之三站台”也是现实中伦敦地铁站的翻版。

在用计算机构建虚拟世界时，通常是把真实的世界数字化。也即，扫描真实世界的物体，得到三维模型，然后发挥一点想象力去修改它，创造一些现实中不存在的奇幻事物。

## 扫描实体世界是什么：城市地图案例
---

说起扫描实体世界，可能马上会想到扫描一个人的三维模型等等，这的确是扫描实体世界的一种。 用城市地图这个大家每天都在用的互联网产品，来说说扫描实体世界、创造数字应用是怎么一步步发展的。

**互联网上的城市地图应用，经历了一个从二维到三维、再到融合现实环境和高度互动的发展演化过程。** 

在互联网普及之前，城市的地图要么是大大的一张，展开可以看到城市的全部区域；要么是厚厚的一本，每个小街区都有详细的分页。这是扫描实体世界在数字化技术普及之前的初级版本——复制到纸上。

有了互联网之后，地图缩进了网页里。我们搜索一个城市的地图，首先会看到它的轮廓。逐步放大之后，我们可以看到每个街区。一开始的时候，网页上的地图只是把纸质地图简单数字化。

之后，街道的信息越来越精细；地图信息的更新越来越方便，甚至我们每个人都可以把信息提交到地图上。

网页地图让我们的出行变得非常方便，不过在很长的时间里，它跟纸质地图功能还非常相似。但接下来，谷歌给地图带来了一些不一样的变化。

它先是发布了谷歌地球，也即卫星地图。在地球周围飞行着众多人造卫星，地球表面的图像资源相当丰富。谷歌将这些已经从太空扫描下来的图像组合起来，变成了谷歌地球服务，让每个人可以方便地查看卫星地图。

真正有趣的是接下来的谷歌街景，这也显示了谷歌的野心，它要从零开始扫描城市的街景。

2007年，谷歌街景正式发布。据说，最初拍摄街景的摄像头就安装在谷歌创始人拉里·佩奇的车顶，汽车每天在街头游走，拍下硅谷各个街道的360度全景图。

之后，谷歌推出了专门的街景拍摄车，全面地将世界各地的街景数字化。

2010年，街景地图正式覆盖了五大洲。两年后，谷歌又推出了街景采集器背包，只要背上它就可以拍摄街道的全景。这样，车辆难以进入的街道就也可以被采集下来了。

谷歌地球和谷歌街景给很多人带来了乐趣，我们不用去某个城市，也可以就像站在它的街头一样，看到自己所处位置四周的街景。

随着移动互联网时代的到来，地图软件在功能上又实现了一次跃升，这就是导航，现在开车时都离不开导航。

这背后有一个你可能没有特别注意的变化， **当一个城市里的几十万人同时使用导航时，智能手机上的传感器成为扫描城市交通状况的新利器。** 地图导航软件可以从用户端获得数据，更精准地反映城市路况信息，它们能够更好地“扫描”城市交通状况。

最初，有人轻而易举地利用漏洞愚弄了谷歌地图的实时交通功能：他搞来几十只手机，全都打开地图导航功能，然后把这些手机放在超市购物篮里，在几乎无人的城市道路上缓慢地拖着走。

就这样，他就在谷歌地图上制造了一个真实世界里其实并不存在的交通拥堵现象。但现在的导航已经变得非常精确了。

近几年，地图仍在高速发展，但发展路径没有变：继续用各种方法在把实体世界中与地图有关的信息都扫描下来，将其数字化并进行处理，然后呈现在地图软件中供用户使用。

地图发展的下一步可能是建立城市的三维模型。

比如，你开车进到超大型停车场的车库，你会希望看到它的内部模型。又比如，未来自动驾驶汽车可能需要更精确的的道路模型，这些模型一部分是预先扫描的，而有些部分是实时扫描的。

总的来说，从纸质地图，到网页地图，再到手机地图App；从普通地图，到卫星地图，再到街景地图和实时地图导航，再到实时扫描的道路和路上物体的三维模型，在城市地图这个领域，正在把“扫描实体世界”这件事做得越来越好。

扫描出的数据也被用来创造出让我们的生活更加便捷的产品。

## 如何扫描实体世界？
---

通过城市地图这样一个产品的演变，看到了“扫描实体世界”的发展过程。但若要建立元宇宙的立体互联网，新的挑战来了： **如何更快、更好、成本更低地获得各种物体的三维模型呢？**

其实，现在要扫描实体世界的物体、获得三维模型已经变得几乎像“拍照”一样简单了。苹果公司甚至已经在消费级产品里提供了不错的解决方案，以它为例，来看看怎么扫描物体的三维模型吧。

举个例子，假设眼前有一只西瓜，要将这只西瓜数字化，大致需要考虑三个层次。

> [!IMPORTANT]
>
> 第一个层次是平面照片，拍一张西瓜的照片。现在要获得一张高清的照片已经不难，市面上已经有一亿像素的中画幅消费级相机。另外，现在有了价值数千元的消费级360度全景相机，全景照片的拍摄变得相当容易。

相应地，360度全景照片也开始被很多行业应用起来，比如用于看房、旅游、展览等等。

> [!IMPORTANT]
>
> 第二个层次是三维图像。要拍摄西瓜的三维图像，然后用独特的方式把这一组图片在你的手机屏幕上呈现出来，要让西瓜图像在你手机上转动，方便看到西瓜的每一个角度。

这其实是一系列的表面图像，有的技术平台将这样的物体表面图像模型称为 **“环物照片”**，环绕四周的环，物体的物，还挺形象的。

再往前略走一步就是去获得物体的**外部三维模型**。iPhone手机的“物体捕捉”（Object Capture）的功能。

借助此功能，你不只能看到西瓜的三维图像，还能了解它的三维数据，比方说，你能知道这只西瓜在某个地方有个小小的凹陷。这个凹陷在普通的灯光照射下看不出来，但若把灯光换个角度，你能看到因为凹陷产生的阴影。

这就给应用带来了新的可能性。比如，我们可以用新的材质把这一只西瓜的表面重新制作出来，放到不同的场景中，放在不同的灯光下。

> [!IMPORTANT]
>
> 第三个层次是真正的三维模型。不仅要得到西瓜外部的、立体的样子，得到外部的精确数据。还想进到西瓜内部，看清每个西瓜籽在什么地方。当然，目前只有在进行**科研和产品设计**时，才会用到这样有内外部结构的三维模型。

在当前元宇宙的应用上，能够获得物体的外部三维图像和形状数据就足够，有了它，我们就足以创造出欺骗人眼的数字物品。

> [!IMPORTANT]
>
> 在1957年，明斯基发明了一种名叫“共聚焦显微成像（Confocal Microscopy）”的技术并申请了专利。
>
> 这个技术是现在广泛应用的 **“共聚焦激光扫描显微（CLSM）”** 背后的基本原理。
> 
> 用光学分层的方式，按照深度一层层进行扫描，最终形成一个物体的三维外部模型。
>
> 现在，自动驾驶汽车在识别三维物体的时候，就是这个技术在发挥作用。没错， **原理早就有了，真正的难题是如何在低成本、快速地用这些技术原理获得三维模型。**

苹果2021年新提供的开发者SDK [RealityKit](https://developer.apple.com/documentation/realitykit) 已经让“物体捕捉”变得非常容易。

比如，拿着iPhone手机围绕一个西瓜走一圈，拍下它全方位的照片，也即获得一组图像。然后，在苹果电脑上用对应的摄影测量API（photogrammetry） 对这些特殊的照片进行处理，就可以获得一个物体的外部三维模型。

之后，可以用相应的工具给它增加光线、阴影或其他视觉效果，也可以把这个三维模型导入到Unity3D等软件中去使用。

若想要获得**非常精确的三维模型**，用你的手机也可以做到，只是你可能需要一些其他的辅助手段。

- 还是以拍摄西瓜为例，需要布置好均匀的灯光。我们不是拿着手机围着它走一圈，而是把手机固定下来，让西瓜在自动旋转的托盘上匀速、缓慢地移动。

- 还需要从多个不同的高度围绕西瓜进行拍摄。

- 之后，就可以用这些图片计算出非常精确的三维模型。

到现在为止，利用苹果的iPhone硬件和它提供的SDK程序，在扫描一个物体的三维模型上，每个人都已经可以做很多了。

- 用 [物体捕捉](https://developer.apple.com/augmented-reality/object-capture) 和摄影测量API来获得物体的三维模型。

- 用苹果的ARKit，来捕捉和显示眼前的实体世界。

- 把实体世界和处理过的三维模型混合起来，形成增强现实的体验。

利用苹果提供的各种工具再往前再走一步。

现在，iPhone手机里都有一个名叫TrueDepth的深度相机系统，该相机系统会投射出红外光点，帮助我们获得物体的三维形状数据。有了它的辅助，你可以按照自己的想法对三维模型进行更加精细的处理。

因此，借助这些工具，把一个数字版的西瓜放在现实世界中你家的餐桌上，打开餐桌上的吊灯，你可以看到西瓜被照亮。凑近了看，你发现自己遮住了某个角度的光，西瓜上的阴影变了。

还可以再进一步，写程序让西瓜滚动起来，或者脑洞大开，让西瓜突然长出嘴来……能做什么完全取决于你的想象力。

<img width="475" height="299" alt="image" src="https://github.com/user-attachments/assets/6406f7eb-8915-4117-b4a3-aec5a9bf89fb" />

手上的消费级产品已经有了扫描物体、形成三维模型的能力。只要你能有效地运用这些功能，它们马上就可以发挥作用。

徕卡就提供了一种名为Leica Aibot AX20的**摄影测量相机**，每秒可以投射36万个激光扫描设定点。将它装在无人机上可以对整个街区进行拍摄，精确地获得街区所有建筑物的三维模型。

若你想要把外滩放进元宇宙，让人可以走在元宇宙的外滩里，你可能就需要这样专业级的系统来扫描实体世界了。

在实验室中，我们可以在人身上装上大量传感器，捕捉人的三维形态和动作，形成精确的、可活动的人物三维模型。用这个模型来做一个数字虚拟人，把它放到元宇宙里，之后，你的朋友哪怕远在天边，也能和你“面对面”聊天。

## 加上环境变化：微软模拟飞行
---

要构建一个像实体世界一样的虚拟世界，除了精确的三维物体之外，有时候还要将外部环境动态地模拟出来，这就涉及计算机模拟或者计算机仿真这个专业领域。

在消费级产品中，我们也能够接触到一些了。比如微软的游戏《模拟飞行》在这方面就做得很有特点。顾名思义，《模拟飞行》的核心功能是让我们从飞行员的视角驾驶飞机，操控飞机的飞行。

《模拟飞行》和普通游戏又有一大不同——它会尽可能地模拟现实的环境。

- 你可以在真实的机场跑道上，比如纽约肯尼迪机场国际机场的跑道上起飞。

- 你可以沿着民航飞机的实际航道飞行，看到“真实”的空中交通状态。

- 飞机外的天气也是实时的，包括风速、方向、温度、湿度、雨水、亮度等都可以跟这个时刻外界的情况一模一样。

- 你还可以从飞行员的视角看到地球上的建筑、道路与山脉，你看到的地球上的树木、建筑都跟现实中一模一样。《模拟飞行》里做了2万亿棵树、15亿座建筑，或许在模拟飞行时，某一刻你就突然看到一棵你爬山时遇到过的树。

《模拟飞行》最大限度地把现实世界复制到了游戏里，它也因此吸引了很多航空爱好者。

若从元宇宙的视角看《模拟飞行》，我们会发现它做的这些刚好就是我们模拟实体世界的关键一步。

- 想要创造一个比真实还真实的元宇宙，就需要获得实体世界的实时环境数据，然后将它呈现在用户眼前：

- 去元宇宙中的亚马逊雨林探险时，我们会希望那里的雨水和真实的亚马逊雨林一样，同时开始同时结束。

- 要想构建一个让人感觉真实的数字世界，这种对环境的捕捉与模拟是必要的。

另外，除了模拟飞行航线、地球表面、天气环境之外，《模拟飞行》这款游戏处理数据的方式也可能对我们接下来实现元宇宙有一定的启发。

《模拟飞行》中的天气数据是实时更新的，也即说，它不是模拟一个静态的世界，而是模拟了一个随着时间变化的现实世界。

这种实时性意味着它的数据不是预先下载的，而是**本地数据和实时数据**二者的混合，也即所谓的**“本地数据+数据流”**的方式，这可能是将来元宇宙的主流数据传输方式。

## 总结
---

构建一个三维立体的互联网，需要扫描实体世界、渲染数字世界和操控数字世界三个层面的技术。通过网络地图的例子看到了扫描实体世界的技术与应用是如何快速进化的。

之后，讨论了扫描实体世界的三个层次：平面照片、三维图像、三维模型。以苹果现在提供的增强现实套件等技术为例，每个人都已经可以用手机扫描出一个物体的立体三维模型。

## 精彩点评
---
